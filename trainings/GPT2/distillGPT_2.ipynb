{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "nQSPr3_NYG-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1PF4KUMirub",
        "outputId": "93ced438-3c96-47c5-f7c2-6d4223edcd38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r0SxfDxaYFrq"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poem_file = open('/content/poem.txt','r')\n",
        "poem = poem_file.read()"
      ],
      "metadata": {
        "id": "0QTQZYaUbiej"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poem_corpus = poem.split(\"\\n\")\n",
        "print(poem_corpus[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHp8Xa7diwgp",
        "outputId": "7ae4495e-f79a-439f-acb1-d358d04eaf33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‡§®‡§õ‡§æ‡§°‡•Ä ‡§ú‡§æ‡§®‡•ã‡§∏‡•ç ‡§π‡•á ‡§Æ‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡§æ‡§£ ! ‡§Ö‡§ï‡•á‡§≤‡•Ä ‡§Æ‡§≤‡§æ‡§à,', '‡§Æ‡§®‡§ï‡•ã ‡§µ‡§®‡§Æ‡§æ ‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à !', '‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à,', '‡§≤‡•ã‡§ö‡§®‡§ï‡§æ ‡§§‡§æ‡§∞‡§æ ! ‡§π‡•á ‡§Æ‡•á‡§∞ ‡§™‡•ç‡§Ø‡§æ‡§∞‡§æ ! ‡§Ø‡•ã ‡§ú‡•ã‡§§‡§ø  ‡§¨‡§ø‡§≤‡§æ‡§è !', '‡§ï‡•á ‡§≠‡§®‡•Ç‡§Å? ‡§≠‡§®‡•ç‡§®‡•á ‡§Æ ‡§ï‡•á‡§π‡•Ä ‡§•‡§ø‡§á‡§®  ‡§µ‡§ø‡§∑ ‡§®‡•à ‡§™‡§ø‡§≤‡§æ‡§è !']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_noise(sentences):\n",
        "    punctuations = ['\\n','\\ufeff','0','1','2','3','4','5','6','7','8','9','‡•¶','‡•ß','‡•®','‡•©','‡•™','‡•´','‡•¨','‡•≠','‡•Æ','‡•Ø','‡•ß‡•¶','\\u200d']\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        for punct in punctuations:\n",
        "            sentence = sentence.replace(punct,'')\n",
        "        processed_sentences.append(sentence)\n",
        "\n",
        "    return processed_sentences"
      ],
      "metadata": {
        "id": "bOs8cDouix6v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_poem_corpus = remove_noise(poem_corpus)\n",
        "print(processed_poem_corpus[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZyBS37xizUT",
        "outputId": "d9d41e43-2cc7-411d-df81-c1a4318c22ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‡§®‡§õ‡§æ‡§°‡•Ä ‡§ú‡§æ‡§®‡•ã‡§∏‡•ç ‡§π‡•á ‡§Æ‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡§æ‡§£ ! ‡§Ö‡§ï‡•á‡§≤‡•Ä ‡§Æ‡§≤‡§æ‡§à,', '‡§Æ‡§®‡§ï‡•ã ‡§µ‡§®‡§Æ‡§æ ‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à !', '‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à,', '‡§≤‡•ã‡§ö‡§®‡§ï‡§æ ‡§§‡§æ‡§∞‡§æ ! ‡§π‡•á ‡§Æ‡•á‡§∞ ‡§™‡•ç‡§Ø‡§æ‡§∞‡§æ ! ‡§Ø‡•ã ‡§ú‡•ã‡§§‡§ø  ‡§¨‡§ø‡§≤‡§æ‡§è !', '‡§ï‡•á ‡§≠‡§®‡•Ç‡§Å? ‡§≠‡§®‡•ç‡§®‡•á ‡§Æ ‡§ï‡•á‡§π‡•Ä ‡§•‡§ø‡§á‡§®  ‡§µ‡§ø‡§∑ ‡§®‡•à ‡§™‡§ø‡§≤‡§æ‡§è !']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('processed_poem.txt','w') as f:\n",
        "  for line in processed_poem_corpus:\n",
        "    f.write(line + '\\n')"
      ],
      "metadata": {
        "id": "_ara9q8hi14H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n"
      ],
      "metadata": {
        "id": "UmYnXpr2i5VT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "gTYhvYhsi72v"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to set parameters\n",
        "train_file_path = \"/content/processed_poem.txt\"\n",
        "model_name = 'Sakonii/distilgpt2-nepali'\n",
        "output_dir = '/content/'\n",
        "overwrite_output_dir = True\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 250\n",
        "save_steps = 1000"
      ],
      "metadata": {
        "id": "RGoeYVjnjA2G"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "HxtNVDiWjE_W",
        "outputId": "c30a95a3-4e8b-4d63-95c9-9f0c29966063"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9500' max='9500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9500/9500 33:23, Epoch 250/250]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.879800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.110000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.081100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.066900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.059400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.053700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.052000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.051300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.050300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.049600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.048800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.048200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.047300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.047000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.046500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.046300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = \"/content/\"\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "uO_Xb8OGjuj1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = input()\n",
        "max_len = int(input()) # 20\n",
        "generate_text(sequence, max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWLWaAvzkU1_",
        "outputId": "682e7e22-10fe-456b-ba20-b835e3ab8673"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡§Ö‡§®‡§æ‡§∞ ‡§¶‡§æ‡§®‡§æ ‡§¶‡§æ‡§Å‡§§‡§ï‡§æ\n",
            "100\n",
            "‡§Ö‡§®‡§æ‡§∞ ‡§¶‡§æ‡§®‡§æ ‡§¶‡§æ‡§Å‡§§‡§ï‡§æ ‡§®! ‡§Ü‡§Æ‡§æ! ‡§Æ‡§≤‡§æ‡§à ‡§π‡•á‡§∞ ‡§®! ‡§ï‡§ø‡§® ‡§®‡§ø ‡§ü‡§æ‡§¢‡§æ ‡§π‡•á‡§∞‡•á‡§ï‡•ã ‡§§‡•ç‡§Ø‡§∏‡•ç‡§§‡•ã! ‡§Æ ‡§Ü‡§è‡§Å, ‡§Æ ‡§Ü‡§è‡§Å! ‡§®‡§ú‡§∞ ‡§´‡•á‡§∞‡§ø ‡§´‡§ø‡§∞‡§æ‡§ä ‡§Ø‡§§‡§æ, ‡§∞‡•ã‡§è‡§∞ ‡§ï‡§∞‡§æ‡§è‡§Å! ‡§∞‡•Å‡§® ‡§™‡§æ‡§â‡§Æ‡§æ ‡§Æ ‡§Ü‡§è‡§Å! ‡§Æ‡•Å‡§ü‡•Å‡§Æ‡§æ ‡§ó‡§æ‡§Å‡§†‡•ã ‡§™‡§æ‡§∞‡•á‡§∞ ‡§π‡•á‡§õ‡§∞‡•ç‡§Ø‡•å! ‡§®‡§ú‡§∞ ‡§´‡§ø‡§∞‡•ç‡§Ø‡•ã ‡§®‡§ø! ‡§®‡§õ‡§æ‡§° ‡§Ü‡§Æ‡§æ! ‡§´‡§∞‡•ç‡§ï ‡§®, ‡§´‡§∞‡•ç‡§ï, ‡§ü‡•Å‡§π‡•Å‡§∞‡•ã ‡§Æ‡§∞‡•ç‡§Ø‡•ã ‡§®‡§ø! ‡§Ü‡§Æ‡§æ! ‡§ü‡•Å‡§π‡•Å‡§∞‡•ã ‡§Æ‡§∞‡•ç‡§Ø‡•ã ‡§®‡§ø! ‡§Æ ‡§Ü‡§è‡§Å ‡§Ü‡§Æ‡§æ! ‡§Æ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9VJIg-YkVKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
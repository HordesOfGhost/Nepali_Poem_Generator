{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u71mzdEVpoJM"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYXlXNlsplb9",
        "outputId": "7f422036-d1b1-474b-c311-7f52a29e9a41"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "je-g-H8UwXJq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102qIx0ns3Dh"
      },
      "source": [
        "# Creating Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDB4s_fcptTz",
        "outputId": "c4b7afef-6e5b-400f-8c49-6064246a3f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: [36, 51, 38, 53, 59, 31, 45, 36, 59, 40, 46, 40, 51, 12, 59, 28, 45, 32, 45, 59, 44, 53, 59, 60]\n",
            "Converted back to characters: मेरो नाम विवेक थापा हो ।\n"
          ]
        }
      ],
      "source": [
        "# Character tokenizer\n",
        "\n",
        "class NepaliTokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dictionary = {}\n",
        "        self.reverse_dictionary = {}\n",
        "\n",
        "        # Add the padding token\n",
        "        self.__add_to_dict('<pad>')\n",
        "\n",
        "        # Add Nepali characters to the dictionary\n",
        "        nepali_characters = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ',\n",
        "                             'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n",
        "                             'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', 'ा', 'ि',\n",
        "                             'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ो', 'ौ', '्' , '!', '.', ',', ' ', '।' ]\n",
        "\n",
        "        for char in nepali_characters:\n",
        "            self.__add_to_dict(char)\n",
        "\n",
        "    def __add_to_dict(self, character):\n",
        "        if character not in self.dictionary:\n",
        "            self.dictionary[character] = len(self.dictionary)\n",
        "            self.reverse_dictionary[self.dictionary[character]] = character\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return [self.dictionary.get(c, self.dictionary['<pad>']) for c in text]  # Use '<pad>' token for unknown characters\n",
        "\n",
        "    def character_to_token(self, character):\n",
        "        return self.dictionary.get(character, self.dictionary['<pad>'])\n",
        "\n",
        "    def token_to_character(self, token):\n",
        "        return self.reverse_dictionary.get(token, '<pad>')\n",
        "\n",
        "    def left_pad_tokens(self, tokens, max_length):\n",
        "        padded_tokens = np.pad(tokens, (max_length - len(tokens), 0), 'constant', constant_values=0)\n",
        "        return padded_tokens.tolist()\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.dictionary)\n",
        "\n",
        "# Example usage\n",
        "nepali_tokenizer = NepaliTokenizer()\n",
        "text = \"मेरो नाम विवेक थापा हो ।\"\n",
        "tokens = nepali_tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens back to characters\n",
        "characters = [nepali_tokenizer.token_to_character(token) for token in tokens]\n",
        "print(\"Converted back to characters:\", ''.join(characters))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6exEArkrtmyx"
      },
      "source": [
        "# Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "8YdfHFb-q7Wm"
      },
      "outputs": [],
      "source": [
        "poem_file = open('C:/Users/Ghost/Desktop/gits/Nepali_Poem_Generator/trainings/datasets/poem.txt','r')\n",
        "poem = poem_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "2qRP_Q1Qt9hT"
      },
      "outputs": [],
      "source": [
        "def remove_noise(sentences):\n",
        "    punctuations = ['\\n','\\ufeff','0','1','2','3','4','5','6','7','8','9','०','१','२','३','४','५','६','७','८','९','१०','\\u200d']\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        for punct in punctuations:\n",
        "            sentence = sentence.replace(punct,'')\n",
        "        processed_sentences.append(sentence)\n",
        "\n",
        "    return processed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3WXAgCEuAnV",
        "outputId": "3175412b-9587-434e-ad68-e8f17d5301c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,', 'मनको वनमा ननिभ्ने गरी विरह जलाई !', 'ननिभ्ने गरी विरह जलाई,', 'लोचनका तारा ! हे मेर प्यारा ! यो जोति  बिलाए !', 'के भनूँ? भन्ने म केही थिइन  विष नै पिलाए !']\n",
            "['नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,', 'मनको वनमा ननिभ्ने गरी विरह जलाई !', 'ननिभ्ने गरी विरह जलाई,', 'लोचनका तारा ! हे मेर प्यारा ! यो जोति  बिलाए !', 'के भनूँ? भन्ने म केही थिइन  विष नै पिलाए !']\n"
          ]
        }
      ],
      "source": [
        "poem_corpus = poem.split(\"\\n\")\n",
        "print(poem_corpus[:5])\n",
        "\n",
        "processed_poem_corpus = remove_noise(poem_corpus)\n",
        "print(processed_poem_corpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKupEVCBuFtP"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_poem_corpus_text = ' '.join(processed_poem_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "nP3AHVfvuDoB"
      },
      "outputs": [],
      "source": [
        "nepali_tokenizer = NepaliTokenizer()\n",
        "tokenized_poem_data = []\n",
        "for poem in processed_poem_corpus:\n",
        "  tokenized_poem_data.append(nepali_tokenizer.tokenize(poem))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI-MOP7juO_q",
        "outputId": "73efad16-27b3-42e4-c425-1af01f6c182e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "प्यारा ! विष नै पिलाए !\n",
            "[32, 55, 37, 45, 38, 45, 59, 56, 59, 40, 46, 42, 59, 31, 52, 59, 32, 46, 39, 45, 8, 59, 56]\n",
            "Converted back to characters: प्यारा ! विष नै पिलाए !\n"
          ]
        }
      ],
      "source": [
        "print(processed_poem_corpus[5])\n",
        "print(tokenized_poem_data[5])\n",
        "\n",
        "# Convert tokens back to characters\n",
        "characters = [nepali_tokenizer.token_to_character(token) for token in tokenized_poem_data[5]]\n",
        "print(\"Converted back to characters:\", ''.join(characters))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGXfCCwuvRIQ"
      },
      "source": [
        "# Padding the Tokenized Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "OXW21Md2ukvy"
      },
      "outputs": [],
      "source": [
        "# Example of left-padding the tokens\n",
        "max_length = max([len(x) for x in tokenized_poem_data])\n",
        "\n",
        "for index, tokens in enumerate(tokenized_poem_data):\n",
        "    tokenized_poem_data[index] = nepali_tokenizer.left_pad_tokens(tokens, max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ZwpDs0xJ9R",
        "outputId": "101c5357-828b-447f-dba7-e5e980f3fa51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Converted back to characters: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,\n"
          ]
        }
      ],
      "source": [
        "print(processed_poem_corpus[0])\n",
        "print(tokenized_poem_data[0])\n",
        "\n",
        "# Convert tokens back to characters\n",
        "characters = [nepali_tokenizer.token_to_character(token) for token in tokenized_poem_data[0]]\n",
        "print(\"Converted back to characters:\", ' '.join(characters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBSb1vNxypJ2"
      },
      "source": [
        "# Input Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SbBEyu_qxSZe"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch module that converts tokens into embeddings.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length)\n",
        "    Output dimension is: (batch_size, sequence_length, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, number_of_tokens):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = torch.nn.Embedding(\n",
        "            num_embeddings=number_of_tokens,\n",
        "            embedding_dim=d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGmvcnGF2udw"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "PUS4VQNH1TpG"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the\n",
        "    transformer's input embeddings to provide a sense of position of the sequence elements.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.positional_encoding = self.create_positional_encoding()\n",
        "\n",
        "    def create_positional_encoding(self):\n",
        "        \"\"\"\n",
        "        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize positional encoding matrix\n",
        "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
        "\n",
        "        # Calculate positional encoding for each position and each dimension\n",
        "        for pos in range(self.max_sequence_length):\n",
        "            for i in range(0, self.d_model, 2):\n",
        "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
        "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
        "\n",
        "                if i + 1 < self.d_model:\n",
        "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
        "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
        "\n",
        "        # Convert numpy array to PyTorch tensor and return it\n",
        "        return torch.from_numpy(positional_encoding).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds the positional encoding to the input embeddings at the corresponding positions.\n",
        "        \"\"\"\n",
        "        # Add positional encodings to input embeddings. The \":\" indexing ensures we only add positional encodings up\n",
        "        # to the length of the sequence in the batch. x.size(0) is the batch size, so this is a way to make sure\n",
        "        # we're not adding extra positional encodings.\n",
        "        return x + self.positional_encoding[:x.size(1), :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yJfP2l3i7s"
      },
      "source": [
        "# Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "1VfHa2Ju2sXK"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a self attention layer.\n",
        "    This layer is used in the MultiHeadedSelfAttention module.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, head_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, head_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = head_dimension\n",
        "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the self attention.\n",
        "\n",
        "        x dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "        output dimension is: (batch_size, sequence_length, head_dimension)\n",
        "        mask dimension is: (batch_size, sequence_length)\n",
        "\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n",
        "        query = self.query_layer(x)\n",
        "        key = self.key_layer(x)\n",
        "        value = self.value_layer(x)\n",
        "\n",
        "        # Calculate the attention weights.\n",
        "        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n",
        "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "        # Scale the attention weights.\n",
        "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
        "\n",
        "        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n",
        "        # This will make the softmax output 0 for these values.\n",
        "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
        "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n",
        "        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n",
        "        attention_scores = self.softmax(attention_weights)\n",
        "\n",
        "        # The attention scores are multiplied by the value\n",
        "        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n",
        "        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n",
        "        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n",
        "        return torch.bmm(attention_scores, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Za8c1V6cj9"
      },
      "source": [
        "# MultiHead Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "pTMtX4LV6af2"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a multi head attention layer.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, number_of_heads):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = embedding_dimension // number_of_heads\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        # Create the self attention modules\n",
        "        self.self_attentions = torch.nn.ModuleList(\n",
        "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
        "\n",
        "        # Create a linear layer to combine the outputs of the self attention modules\n",
        "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the multi head attention.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        mask dimensions are: (batch_size, sequence_length)\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "        # Compute the self attention for each head\n",
        "        # self_attention_outputs dimensions are:\n",
        "        # (number_of_heads, batch_size, sequence_length, head_dimension)\n",
        "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
        "\n",
        "        # Concatenate the self attention outputs\n",
        "        # self_attention_outputs_concatenated dimensions are:\n",
        "        # (batch_size, sequence_length, number_of_heads * head_dimension)\n",
        "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
        "\n",
        "        # Apply the output layer to the concatenated self attention outputs\n",
        "        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        return self.output_layer(concatenated_self_attention_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0AIWz7n-fa2"
      },
      "source": [
        "# Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "292Ysesq7RV9"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for an encoder layer.\n",
        "\n",
        "    An encoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
        "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n",
        "        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the encoder layer.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        mask dimensions are: (batch_size, sequence_length)\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # Layer normalization 1\n",
        "        normalized_x = self.layer_normalization_1(x)\n",
        "\n",
        "        # Multi headed self attention\n",
        "        attention_output = self.multi_headed_self_attention(normalized_x, mask)\n",
        "\n",
        "        # Residual output\n",
        "        residual_output = x + attention_output\n",
        "\n",
        "        # Layer normalization 2\n",
        "        normalized_residual_output = self.layer_normalization_2(residual_output)\n",
        "\n",
        "        # Feed forward\n",
        "        feed_forward_output = self.feed_forward(normalized_residual_output)\n",
        "\n",
        "        # Dropout, only when training.\n",
        "        if self.training:\n",
        "            feed_forward_output = self.dropout(feed_forward_output)\n",
        "\n",
        "        # Residual output\n",
        "        return residual_output + feed_forward_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "7uuPRqS0-hEi"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a stack of decoders.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_layers,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate,\n",
        "            max_sequence_length\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "        # Create the encoder layers\n",
        "        self.encoder_layers = torch.nn.ModuleList(\n",
        "            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
        "             range(number_of_layers)])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        decoder_outputs = x\n",
        "        for decoder_layer in self.encoder_layers:\n",
        "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
        "\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "NvmK93zH-oCZ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a feed forward layer.\n",
        "\n",
        "    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
        "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the feed forward layer.\n",
        "        \"\"\"\n",
        "        return self.linear_2(torch.relu(self.linear_1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yacQ2qY1-v2p"
      },
      "source": [
        "# Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "KgSdSIDJ-qco"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a language model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            number_of_tokens,  # The number of tokens in the vocabulary\n",
        "            max_sequence_length=512,  # The maximum sequence length to use for attention\n",
        "            embedding_dimension=512,  # The dimension of the token embeddings\n",
        "            number_of_layers=6,  # The number of decoder layers to use\n",
        "            number_of_heads=4,  # The number of attention heads to use\n",
        "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
        "            dropout_rate=0.1  # The dropout rate to use\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.number_of_tokens = number_of_tokens\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        if feed_forward_dimension is None:\n",
        "            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n",
        "            self.feed_forward_dimension = embedding_dimension * 4\n",
        "        else:\n",
        "            self.feed_forward_dimension = feed_forward_dimension\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Create the token embedding layer\n",
        "        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n",
        "\n",
        "        # Create the positional encoding layer\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
        "\n",
        "        # Create the normalization layer\n",
        "        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "        # Create the decoder stack\n",
        "        self.decoder = DecoderStack(\n",
        "            embedding_dimension=embedding_dimension,\n",
        "            number_of_layers=number_of_layers,\n",
        "            number_of_heads=number_of_heads,\n",
        "            feed_forward_dimension=self.feed_forward_dimension,\n",
        "            dropout_rate=dropout_rate,\n",
        "            max_sequence_length=max_sequence_length\n",
        "        )\n",
        "\n",
        "        # Create the language model head\n",
        "        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Compute the token embeddings\n",
        "        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "\n",
        "        # Compute the positional encoding\n",
        "        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        positional_encoding = self.positional_encoding(token_embeddings)\n",
        "\n",
        "        # Post embedding layer normalization\n",
        "        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n",
        "\n",
        "        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n",
        "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
        "\n",
        "        return lm_head_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "d5yKOmb3-xsZ"
      },
      "outputs": [],
      "source": [
        "class LMHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for the language model head.\n",
        "    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, number_of_tokens):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_tokens = number_of_tokens\n",
        "        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the language model head.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
        "        \"\"\"\n",
        "        # Compute the linear layer\n",
        "        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
        "        linear_output = self.linear(x)\n",
        "\n",
        "        return linear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bFSsb2UB8xh"
      },
      "source": [
        "# Autoregressive Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "LiL289bRBHk6"
      },
      "outputs": [],
      "source": [
        "class AutoregressiveWrapper(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module that wraps a GPT model and makes it autoregressive.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gpt_model):\n",
        "        super().__init__()\n",
        "        self.model = gpt_model\n",
        "        self.max_sequence_length = self.model.max_sequence_length\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Autoregressive forward pass\n",
        "        \"\"\"\n",
        "        inp, target = x[:, :-1], x[:, 1:]\n",
        "        mask = mask[:, :-1]\n",
        "\n",
        "        output = self.model(inp, mask)\n",
        "        return output, target\n",
        "\n",
        "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Calculate the token probabilities for the next token in the sequence.\n",
        "        \"\"\"\n",
        "        logits = self.model(x, mask)[:, -1]\n",
        "\n",
        "        # Apply the temperature\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "        # Apply the softmax\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        return probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv5AOb75CFSi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ZVH5dKVaCofZ"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, tokenizer: NepaliTokenizer, optimizer=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        if optimizer is None:\n",
        "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        else:\n",
        "            self.optimizer = optimizer\n",
        "        self.tokenizer = tokenizer\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, data: List[str], epochs, batch_size):\n",
        "        loss_per_epoch = []\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "\n",
        "            # Shuffle the sequences\n",
        "            random.shuffle(data)\n",
        "\n",
        "            # Create batches of sequences and their respective mask.\n",
        "            batches = []\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
        "\n",
        "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
        "                mask_tensor = torch.ones_like(sequence_tensor)\n",
        "                mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<pad>')] = 0\n",
        "\n",
        "                batches.append((sequence_tensor, mask_tensor))\n",
        "\n",
        "            # Train the model on each batch\n",
        "            for batch in batches:\n",
        "                self.model.train()\n",
        "\n",
        "                # Create the input and mask tensors\n",
        "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "\n",
        "                for i, input_entry in enumerate(batch[0]):\n",
        "                    input_tensor[i] = input_entry\n",
        "\n",
        "                for i, mask_entry in enumerate(batch[1]):\n",
        "                    mask_tensor[i] = mask_entry\n",
        "\n",
        "                # Compute the model output\n",
        "                model_output, target = self.model.forward(x=input_tensor, mask=mask_tensor)\n",
        "\n",
        "                # Compute the losses\n",
        "                # The loss is computed on the model output and the target\n",
        "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
        "\n",
        "                # Backpropagate the loss.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the gradients. This is used to prevent exploding gradients.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "\n",
        "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
        "                # are not used in the next step.\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            # Print the loss\n",
        "            epoch_loss = np.average(losses)\n",
        "            loss_per_epoch.append(epoch_loss)\n",
        "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
        "\n",
        "        return loss_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Juqsj8dsB73p",
        "outputId": "e493c2d4-a20a-4633-8c9b-a34dcfa5fab8"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/Users/Ghost/Desktop/gits/Nepali_Poem_Generator/datasets/poem.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[76], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m sequences \u001b[38;5;241m=\u001b[39m create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, tokenizer, optimizer)\n\u001b[0;32m     44\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(sequences, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\optim\\optimizer.py:266\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    263\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_frame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m replay\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdeprecated_func\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_symbolic_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fx_tracing\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_compiling\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_safe_constant, NP_SUPPORTED_MODULES\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\_dynamo\\config.py:49\u001b[0m\n\u001b[0;32m     41\u001b[0m specialize_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Assume these functions return constants\u001b[39;00m\n\u001b[0;32m     44\u001b[0m constant_functions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     45\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39mis_fx_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241m.\u001b[39mis_in_onnx_export: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     50\u001b[0m     external_utils\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m }\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# legacy config, does nothing now!\u001b[39;00m\n\u001b[0;32m     55\u001b[0m dynamic_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\__init__.py:1831\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[0;32m   1830\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m-> 1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\onnx\\__init__.py:46\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckerError  \u001b[38;5;66;03m# Backwards compatibility\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     _optimize_graph,\n\u001b[0;32m     36\u001b[0m     _run_symbolic_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     unregister_custom_op_symbolic,\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort:skip. needs to be last to avoid circular import\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     ExportOptions,\n\u001b[0;32m     48\u001b[0m     ExportOutput,\n\u001b[0;32m     49\u001b[0m     ExportOutputSerializer,\n\u001b[0;32m     50\u001b[0m     dynamo_export,\n\u001b[0;32m     51\u001b[0m     OnnxExporterError,\n\u001b[0;32m     52\u001b[0m     enable_fake_mode,\n\u001b[0;32m     53\u001b[0m     OnnxRegistry,\n\u001b[0;32m     54\u001b[0m     DiagnosticOptions,\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     58\u001b[0m     is_onnxrt_backend_supported,\n\u001b[0;32m     59\u001b[0m     OrtBackend \u001b[38;5;28;01mas\u001b[39;00m _OrtBackend,\n\u001b[0;32m     60\u001b[0m     OrtBackendOptions \u001b[38;5;28;01mas\u001b[39;00m _OrtBackendOptions,\n\u001b[0;32m     61\u001b[0m     OrtExecutionProvider \u001b[38;5;28;01mas\u001b[39;00m _OrtExecutionProvider,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Modules\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbolic_helper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_onnxrt_backend_supported\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    111\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _beartype, io_adapter\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infra\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     decomposition_table,\n\u001b[0;32m     44\u001b[0m     patcher \u001b[38;5;28;01mas\u001b[39;00m patcher,\n\u001b[0;32m     45\u001b[0m     registration,\n\u001b[0;32m     46\u001b[0m     serialization \u001b[38;5;28;01mas\u001b[39;00m fx_serialization,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# We can only import onnx from this module in a type-checking context to ensure that\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 'import torch.onnx' continues to work without having 'onnx' installed. We fully\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 'import onnx' inside of dynamo_export (by way of _assert_dependencies).\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\onnx\\_internal\\fx\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ONNXTorchPatcher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model_with_external_data\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_model_with_external_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNXTorchPatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\poem_gen\\lib\\site-packages\\torch\\onnx\\_internal\\fx\\patcher.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# safetensors is not an exporter requirement, but needed for some huggingface models\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]  # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m safetensors_torch  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     has_safetensors_and_transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\Desktop\\gits\\Nepali_Poem_Generator\\trainings\\transformers\\transformers.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 4\u001b[0m poem_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/Ghost/Desktop/gits/Nepali_Poem_Generator/datasets/poem.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m poem \u001b[38;5;241m=\u001b[39m poem_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      7\u001b[0m poem_corpus \u001b[38;5;241m=\u001b[39m poem\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Ghost/Desktop/gits/Nepali_Poem_Generator/datasets/poem.txt'"
          ]
        }
      ],
      "source": [
        "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
        "    # Create sequences of length max_sequence_length + 1\n",
        "    # The last token of each sequence is the target token\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
        "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
        "    # Tokenize the training data\n",
        "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
        "    for _ in range(max_sequence_length):\n",
        "        # Prepend padding tokens\n",
        "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
        "    return tokenized_training_data\n",
        "\n",
        "\n",
        "tokenizer = NepaliTokenizer(processed_poem_corpus_text)\n",
        "\n",
        "embedding_dimension = 256\n",
        "max_sequence_length = 20\n",
        "number_of_tokens = tokenizer.size()\n",
        "\n",
        "# Create the model\n",
        "model = AutoregressiveWrapper(LanguageModel(\n",
        "    embedding_dimension=embedding_dimension,\n",
        "    number_of_tokens=number_of_tokens,\n",
        "    number_of_heads=4,\n",
        "    number_of_layers=3,\n",
        "    dropout_rate=0.1,\n",
        "    max_sequence_length=max_sequence_length\n",
        "))\n",
        "\n",
        "# Create the training data\n",
        "training_data = '. '.join(processed_poem_corpus)\n",
        "\n",
        "tokenized_and_padded_training_data = tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\n",
        "sequences = create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n",
        "\n",
        "# Train the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "trainer = Trainer(model, tokenizer, optimizer)\n",
        "trainer.train(sequences, epochs=100, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "xpnxRQ0oC-Iv"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[75], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m LanguageModel\u001b[38;5;241m.\u001b[39mload_checkpoint(path)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AutoregressiveWrapper(model)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_checkpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_checkpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "def save_checkpoint(self, path):\n",
        "    print(f'Saving checkpoint {path}')\n",
        "    torch.save({\n",
        "        'number_of_tokens': self.number_of_tokens,\n",
        "        'max_sequence_length': self.max_sequence_length,\n",
        "        'embedding_dimension': self.embedding_dimension,\n",
        "        'number_of_layers': self.number_of_layers,\n",
        "        'number_of_heads': self.number_of_heads,\n",
        "        'feed_forward_dimension': self.feed_forward_dimension,\n",
        "        'dropout_rate': self.dropout_rate,\n",
        "        'model_state_dict': self.state_dict()\n",
        "    }, path)\n",
        "\n",
        "@staticmethod\n",
        "def load_checkpoint(path) -> 'LanguageModel':\n",
        "    checkpoint = torch.load(path)\n",
        "    model = LanguageModel(\n",
        "        number_of_tokens=checkpoint['number_of_tokens'],\n",
        "        max_sequence_length=checkpoint['max_sequence_length'],\n",
        "        embedding_dimension=checkpoint['embedding_dimension'],\n",
        "        number_of_layers=checkpoint['number_of_layers'],\n",
        "        number_of_heads=checkpoint['number_of_heads'],\n",
        "        feed_forward_dimension=checkpoint['feed_forward_dimension'],\n",
        "        dropout_rate=checkpoint['dropout_rate']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n",
        "\n",
        "def save_checkpoint(self, path):\n",
        "    self.model.save_checkpoint(path)\n",
        "\n",
        "@staticmethod\n",
        "def load_checkpoint(path) -> 'AutoregressiveWrapper':\n",
        "    model = LanguageModel.load_checkpoint(path)\n",
        "    return AutoregressiveWrapper(model)\n",
        "\n",
        "model.save_checkpoint('./trained_model')\n",
        "model = model.load_checkpoint('./trained_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

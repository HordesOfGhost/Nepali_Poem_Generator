{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQSPr3_NYG-U"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1PF4KUMirub",
        "outputId": "93ced438-3c96-47c5-f7c2-6d4223edcd38"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r0SxfDxaYFrq"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0QTQZYaUbiej"
      },
      "outputs": [],
      "source": [
        "poem_file = open('/content/poem.txt','r')\n",
        "poem = poem_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHp8Xa7diwgp",
        "outputId": "7ae4495e-f79a-439f-acb1-d358d04eaf33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['‡§®‡§õ‡§æ‡§°‡•Ä ‡§ú‡§æ‡§®‡•ã‡§∏‡•ç ‡§π‡•á ‡§Æ‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡§æ‡§£ ! ‡§Ö‡§ï‡•á‡§≤‡•Ä ‡§Æ‡§≤‡§æ‡§à,', '‡§Æ‡§®‡§ï‡•ã ‡§µ‡§®‡§Æ‡§æ ‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à !', '‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à,', '‡§≤‡•ã‡§ö‡§®‡§ï‡§æ ‡§§‡§æ‡§∞‡§æ ! ‡§π‡•á ‡§Æ‡•á‡§∞ ‡§™‡•ç‡§Ø‡§æ‡§∞‡§æ ! ‡§Ø‡•ã ‡§ú‡•ã‡§§‡§ø  ‡§¨‡§ø‡§≤‡§æ‡§è !', '‡§ï‡•á ‡§≠‡§®‡•Ç‡§Å? ‡§≠‡§®‡•ç‡§®‡•á ‡§Æ ‡§ï‡•á‡§π‡•Ä ‡§•‡§ø‡§á‡§®  ‡§µ‡§ø‡§∑ ‡§®‡•à ‡§™‡§ø‡§≤‡§æ‡§è !']\n"
          ]
        }
      ],
      "source": [
        "poem_corpus = poem.split(\"\\n\")\n",
        "print(poem_corpus[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bOs8cDouix6v"
      },
      "outputs": [],
      "source": [
        "def remove_noise(sentences):\n",
        "    punctuations = ['\\n','\\ufeff','0','1','2','3','4','5','6','7','8','9','‡•¶','‡•ß','‡•®','‡•©','‡•™','‡•´','‡•¨','‡•≠','‡•Æ','‡•Ø','‡•ß‡•¶','\\u200d']\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        for punct in punctuations:\n",
        "            sentence = sentence.replace(punct,'')\n",
        "        processed_sentences.append(sentence)\n",
        "\n",
        "    return processed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZyBS37xizUT",
        "outputId": "d9d41e43-2cc7-411d-df81-c1a4318c22ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['‡§®‡§õ‡§æ‡§°‡•Ä ‡§ú‡§æ‡§®‡•ã‡§∏‡•ç ‡§π‡•á ‡§Æ‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡§æ‡§£ ! ‡§Ö‡§ï‡•á‡§≤‡•Ä ‡§Æ‡§≤‡§æ‡§à,', '‡§Æ‡§®‡§ï‡•ã ‡§µ‡§®‡§Æ‡§æ ‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à !', '‡§®‡§®‡§ø‡§≠‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§ú‡§≤‡§æ‡§à,', '‡§≤‡•ã‡§ö‡§®‡§ï‡§æ ‡§§‡§æ‡§∞‡§æ ! ‡§π‡•á ‡§Æ‡•á‡§∞ ‡§™‡•ç‡§Ø‡§æ‡§∞‡§æ ! ‡§Ø‡•ã ‡§ú‡•ã‡§§‡§ø  ‡§¨‡§ø‡§≤‡§æ‡§è !', '‡§ï‡•á ‡§≠‡§®‡•Ç‡§Å? ‡§≠‡§®‡•ç‡§®‡•á ‡§Æ ‡§ï‡•á‡§π‡•Ä ‡§•‡§ø‡§á‡§®  ‡§µ‡§ø‡§∑ ‡§®‡•à ‡§™‡§ø‡§≤‡§æ‡§è !']\n"
          ]
        }
      ],
      "source": [
        "processed_poem_corpus = remove_noise(poem_corpus)\n",
        "print(processed_poem_corpus[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_ara9q8hi14H"
      },
      "outputs": [],
      "source": [
        "with open('processed_poem.txt','w') as f:\n",
        "  for line in processed_poem_corpus:\n",
        "    f.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UmYnXpr2i5VT"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gTYhvYhsi72v"
      },
      "outputs": [],
      "source": [
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RGoeYVjnjA2G"
      },
      "outputs": [],
      "source": [
        "# you need to set parameters\n",
        "train_file_path = \"/content/processed_poem.txt\"\n",
        "model_name = 'Sakonii/distilgpt2-nepali'\n",
        "output_dir = '/content/'\n",
        "overwrite_output_dir = True\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 250\n",
        "save_steps = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "HxtNVDiWjE_W",
        "outputId": "c30a95a3-4e8b-4d63-95c9-9f0c29966063"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9500' max='9500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9500/9500 33:23, Epoch 250/250]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.879800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.110000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.081100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.066900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.059400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.053700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.052000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.051300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.050300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.049600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.048800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.048200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.047300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.047000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.046500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.046300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uO_Xb8OGjuj1"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = \"C:/Users/Ghost/Desktop/gits/Nepali_Poem_Generator/trainings/GPT2/models/dGPT2\"\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWLWaAvzkU1_",
        "outputId": "682e7e22-10fe-456b-ba20-b835e3ab8673"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\transformers\\generation\\utils.py:1363: UserWarning: Input length of input_ids is 93, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‡§Æ‡•á‡§∞‡•ã ‡§Æ‡§®‡§Æ‡§æ ‡§ú‡§æ‡§ó‡•á‡§ï‡•ã, ‡§ù‡§≤‡•ç‡§ù‡§≤‡•Ä ‡§¶‡•á‡§ñ‡•Ä ‡§µ‡§ø‡§∞‡§π ‡§≤‡§æ‡§ó‡•á‡§ï‡•ã! ‡§µ‡§ö‡§® ‡§§‡§ø‡§Æ‡•ç‡§∞‡•ã ‡§§‡§æ‡§∞‡§Æ‡§æ ‡§Æ‡§®‡§ï‡•ã ‡§®‡§ø‡§¶‡§æ‡§á‡§∞‡§π‡§®‡•ç‡§õ, ‡§∏‡§Æ‡•ç‡§ù‡•á‡§∞ ‡§Ü‡§Ø‡•ã ‡§ù‡§®‡•ç‡§ï‡§®‡•ç‡§õ ‡§≠‡§ø‡§§‡•ç‡§∞, ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ï‡§π‡§®‡•ç‡§õ, ‡§¶‡•Å‡§É‡§ñ‡§ï‡•ã ‡§ï‡§æ‡§®‡§Æ‡§æ ‡§∏‡•Å‡§ñ‡§ï‡•ã ‡§ï‡§•‡§æ ‡§¨‡§ú‡§æ‡§á‡§∞‡§π‡§®‡•ç‡§õ ‡•§ ‡§™‡§ñ‡•á‡§ü‡§æ ‡§õ‡•à‡§®‡§®‡•ç ‡§â‡§°‡•á‡§∞ ‡§ú‡§æ‡§® ‡§ö‡§ø‡§°‡§ø‡§Ø‡§æ ‡§â‡§°‡•á‡§ï‡§æ, ‡§π‡•á‡§∞‡•á‡§∞ ‡§¨‡§∏‡•Ä ‡§Ü‡§Å‡§∏‡•Å‡§ï‡§æ ‡§•‡•ã‡§™‡§æ ‡§ó‡§π‡§Æ‡§æ ‡§õ‡•Å‡§ü‡•á‡§ï‡§æ, ‡§¶‡•á‡§ñ‡•á‡§®‡•å ‡§§‡§ø‡§Æ‡•ç‡§≤‡•á ‡§ï‡§§‡§ø‡§ï‡§æ ‡§•‡§ø‡§è ‡§õ‡§æ‡§§‡•Ä‡§Æ‡§æ ‡§ó‡•Å‡§°‡•á‡§ï‡§æ! ‡§ï‡§ø‡§® ‡§π‡•ã ‡§ï‡§ø‡§®, ‡§Ø‡•ã ‡§Æ‡•á‡§∞‡•ã ‡§Æ‡§® ‡§¨‡§æ‡§¶‡§≤‡§≤‡•á ‡§¢‡§æ‡§ï‡•ç‡§¶‡§õ\n"
          ]
        }
      ],
      "source": [
        "sequence = input()\n",
        "max_len = int(input()) # 20\n",
        "generate_text(sequence, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9VJIg-YkVKE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

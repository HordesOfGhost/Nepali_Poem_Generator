{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "R9Wel5CbZ3Jx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oBP7yYofZzkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e67196-0462-4fd0-ee2b-03d53df89016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ZA4L8GjQbQ8z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read Text File**"
      ],
      "metadata": {
        "id": "0IXvC6ifdEIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_file = open('/content/drive/MyDrive/poem/datasets/poem.txt','r')\n",
        "poem = poem_file.read()"
      ],
      "metadata": {
        "id": "P5SVtaUidCpw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read Word2Vec Model**"
      ],
      "metadata": {
        "id": "IcjNaznxdJVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2vec_model= Word2Vec.load(\"/content/drive/MyDrive/language_models/nepaliW2V_5Million.model\")"
      ],
      "metadata": {
        "id": "eTN93nnLhcTM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "IBbioQVseAnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read numbers"
      ],
      "metadata": {
        "id": "72gmZQqTfUG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nepali_num_file=open(\"/content/drive/MyDrive/poem/preprocess/numbers.txt\",\"r\",encoding=\"utf-8\")\n",
        "nepali_num=nepali_num_file.read()\n",
        "nepali_num=nepali_num.split(\",\")"
      ],
      "metadata": {
        "id": "WmEfXOqreAOV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split"
      ],
      "metadata": {
        "id": "kYp-LamBfWxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_corpus = poem.split(\"\\n\")\n",
        "print(poem_corpus[:5])"
      ],
      "metadata": {
        "id": "U9OoXBl7fKDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426963a7-92fa-44b2-e37a-f67158c9b362"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,', 'मनको वनमा ननिभ्ने गरी विरह जलाई !', 'ननिभ्ने गरी विरह जलाई,', 'लोचनका तारा ! हे मेर प्यारा ! यो जोति  बिलाए !', 'के भनूँ? भन्ने म केही थिइन  विष नै पिलाए !']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Denoising"
      ],
      "metadata": {
        "id": "Vyc5ez5Afmbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_puncutations_and_noise(sentences):\n",
        "    punctuations_and_noise = ['।', ',', ';', '?', ' !',' ! ' '!', '—', '-', '.',\"’\",\"‘\",\"'\",\"”\",'\\u200d']\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        for punct in punctuations_and_noise:\n",
        "            sentence = sentence.replace(punct,'')\n",
        "        processed_sentences.append(sentence)\n",
        "\n",
        "    return processed_sentences"
      ],
      "metadata": {
        "id": "D4d7X8AkfLQT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_poem_corpus = remove_puncutations_and_noise(poem_corpus)\n",
        "print(processed_poem_corpus[:5])"
      ],
      "metadata": {
        "id": "EP5Rrr_nfMwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6582103a-a9dc-4015-9afb-3968792ce8e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['नछाडी जानोस् हे मेरा प्राण अकेली मलाई', 'मनको वनमा ननिभ्ने गरी विरह जलाई', 'ननिभ्ने गरी विरह जलाई', 'लोचनका तारा हे मेर प्यारा यो जोति  बिलाए', 'के भनूँ भन्ने म केही थिइन  विष नै पिलाए']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make Corpus Ready to Fit**"
      ],
      "metadata": {
        "id": "kuYBctkZgV3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import snowballstemmer\n",
        "mainlist = list()\n",
        "class Main_Data_list:\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.noise_list = ['\\n','\\ufeff','0','1','2','3','4','5','6','7','8','9','०','१','२','३','४','५','६','७','८','९','१०','।', ',', ';', '?', ' !', \"”\",' ! ' '!', '—', '-', '.',\"’\",\"‘\",\"'\",'\\u200d']\n",
        "        self.mainlist = []\n",
        "\n",
        "        self.stemmer = snowballstemmer.NepaliStemmer()\n",
        "\n",
        "    def simple_tokenizer(self,text) -> list:\n",
        "\n",
        "        line = re.sub('[।]',\"\", text)\n",
        "\n",
        "        devanagari_range = r'[\\u0900-\\u097F\\\\]'\n",
        "        def getDevanagariCharCount(token):\n",
        "            return len(list(filter(lambda char: re.match(devanagari_range, char), (char for char in token))))\n",
        "        def isDevanagari(token):\n",
        "            return True if getDevanagariCharCount(token) >= len(token)/2 else False\n",
        "\n",
        "        tokens = list(filter(lambda t: isDevanagari(t), line.split(\" \")))\n",
        "        return tokens\n",
        "\n",
        "    def get(self):\n",
        "        for i,line in enumerate(self.dataset[0:2000000]):\n",
        "\n",
        "            wordsList = self.simple_tokenizer(line)\n",
        "            words1 = [w for w in wordsList if not w in self.noise_list]\n",
        "            words1.append('')\n",
        "            words = []\n",
        "            for word in words1:\n",
        "              words.append([word.replace(noise,'') for noise in self.noise_list][0])\n",
        "              self.mainlist.append(words)\n",
        "\n",
        "            if i % 100000 == 0:\n",
        "                print(f\"DONE FOR {i/100000} LAKHS LINES\")\n",
        "        return self.mainlist\n",
        "\n",
        "final = Main_Data_list(processed_poem_corpus)\n",
        "mainlist = final.get()"
      ],
      "metadata": {
        "id": "gCaysagtgVZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9cb04d-bfb1-43bd-898e-46f0a1220857"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE FOR 0.0 LAKHS LINES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fit Word2Vec Model**"
      ],
      "metadata": {
        "id": "qO2ceWRBdq_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2vec_model.build_vocab(mainlist, update=True)\n",
        "w2vec_model.train(mainlist, total_examples=w2vec_model.corpus_count, epochs=w2vec_model.epochs)"
      ],
      "metadata": {
        "id": "J_phWSPrdkco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031841d4-0a0a-41ec-ff51-425a1376d546"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(612288, 780460)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_weights = w2vec_model.wv.vectors\n",
        "vocab_size, emdedding_size = trained_weights.shape\n",
        "vocab_size, emdedding_size"
      ],
      "metadata": {
        "id": "t0C6aP-wZFdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634cd30f-f16b-4927-ba5a-841a491d5595"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(293154, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Train dataset**"
      ],
      "metadata": {
        "id": "FApk0jHG9_40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "oe38R3O9Clp6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "embedding_list = []\n",
        "for line in processed_poem_corpus:\n",
        "  embedding_list.append([w2vec_model.wv[word].astype(np.float32) for word in line.split()])\n",
        "\n",
        "max_sequence_len = max([len(x) for x in embedding_list])\n",
        "\n",
        "for embeddings in embedding_list:\n",
        "  for i in range(1, len(embeddings)):\n",
        "    embedding_seq = embeddings[: i+1]\n",
        "    padded_sequence = [torch.zeros(1)] * max_sequence_len\n",
        "\n",
        "    for index,_ in enumerate(embedding_seq):\n",
        "      insert_index = max_sequence_len - len(embedding_seq) + index\n",
        "      padded_sequence[insert_index] = embedding_seq[index]\n",
        "\n",
        "    input_sequences.append(padded_sequence)\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "predictors, label = input_sequences[:, :-1],input_sequences[:, -1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2r42SWpa77J",
        "outputId": "78013167-124f-470e-abb0-7b32fe05ac36"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-548feeca49f1>:19: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  input_sequences = np.array(input_sequences)\n",
            "<ipython-input-53-548feeca49f1>:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  input_sequences = np.array(input_sequences)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_poem_corpus[1])\n",
        "similar_words = w2vec_model.wv.similar_by_vector( np.array(predictors[6][-1]), topn=1)\n",
        "similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfjPL3wvehUg",
        "outputId": "dbf7fde1-f5fc-404b-a56a-140da454d900"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मनको वनमा ननिभ्ने गरी विरह जलाई\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('मनको', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predictors = tf.convert_to_tensor(predictors, dtype=tf.float32)\n",
        "# labels = tf.convert_to_tensor(label, dtype=tf.float32)\n",
        "predictors[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FKOPvC3cX_v",
        "outputId": "059e6420-273d-4e91-8a38-f32f4b755065"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([tensor([0.]), tensor([0.]), tensor([0.]), tensor([0.]),\n",
              "       tensor([0.]), tensor([0.]), tensor([0.]), tensor([0.]),\n",
              "       tensor([0.]),\n",
              "       array([ 2.21316218e-01, -6.84821904e-02,  6.40273392e-02, -4.64315936e-02,\n",
              "              -1.26503855e-01, -5.76133840e-02, -8.57502818e-02,  8.41274783e-02,\n",
              "               4.53876704e-03, -1.67515144e-01, -1.61734328e-01, -8.42714589e-03,\n",
              "               5.61235882e-02,  1.12102836e-01, -1.84491219e-03, -4.68855798e-02,\n",
              "               8.88644904e-03,  7.69452080e-02,  9.80778784e-02, -1.45451277e-02,\n",
              "               6.41386956e-02, -4.01973054e-02, -8.15697946e-03,  5.98928407e-02,\n",
              "               1.29324328e-02, -2.61404157e-01,  1.61195174e-01, -1.89326387e-02,\n",
              "              -4.45985189e-03,  9.77275446e-02,  6.17768727e-02, -1.34935202e-02,\n",
              "               1.58869587e-02,  1.79507267e-02, -2.71560202e-05, -2.08216831e-01,\n",
              "              -2.52208710e-02,  8.00456181e-02,  4.35174964e-02,  4.54146713e-02,\n",
              "               6.95537627e-02,  1.28074691e-01, -9.31303203e-02,  3.73181850e-02,\n",
              "              -9.68044773e-02, -1.42880389e-02, -1.61383212e-01, -4.98270132e-02,\n",
              "               5.59171364e-02, -6.60162196e-02, -9.50140655e-02,  1.07049748e-01,\n",
              "              -1.99064955e-01,  2.57802960e-02,  4.41010529e-03,  1.93717033e-02,\n",
              "              -1.00238444e-02,  4.35315557e-02,  7.32688010e-02, -6.79157600e-02,\n",
              "              -5.20744547e-02, -5.55584505e-02, -5.17618237e-03, -1.01045175e-02,\n",
              "              -6.25253245e-02, -3.85185219e-02,  7.83316442e-04, -1.71404153e-01,\n",
              "              -1.37268782e-01, -1.56507775e-01, -1.34911552e-01,  1.42928943e-01,\n",
              "               4.92657088e-02, -6.13697544e-02, -7.12809637e-02, -2.99549639e-01,\n",
              "               1.29535779e-01, -4.28040028e-02,  1.65509451e-02, -3.00994545e-01,\n",
              "               7.56212026e-02,  5.64957298e-02,  1.07566856e-01,  7.97276909e-04,\n",
              "               7.88794905e-02, -3.91667150e-02,  9.43234637e-02, -8.97304043e-02,\n",
              "               9.93317459e-03,  4.88831438e-02,  6.15482964e-02,  7.35292658e-02,\n",
              "              -4.95815948e-02, -6.77813664e-02,  2.28120331e-02, -1.79649424e-02,\n",
              "               6.53499663e-02,  3.02115874e-03,  4.89344858e-02,  4.69691865e-02,\n",
              "               9.07430425e-03,  1.81208462e-01,  5.32407500e-02, -7.16050938e-02,\n",
              "              -1.10997455e-02,  2.57520210e-02,  8.55089128e-02,  1.49251670e-01,\n",
              "               6.28468543e-02, -1.78110525e-01,  9.05277394e-03, -1.60465658e-01,\n",
              "               7.85349235e-02, -5.33318818e-02, -9.69188511e-02,  7.80781917e-03,\n",
              "               1.60989299e-01,  7.74807408e-02, -1.49373831e-02,  1.30975410e-01,\n",
              "               3.27924043e-02, -6.60974756e-02,  1.45219579e-01, -1.84377983e-01,\n",
              "              -1.92308262e-01,  1.46875381e-01, -2.34358981e-01,  9.42645967e-02,\n",
              "               2.64870971e-02, -2.62052417e-02,  2.07534954e-01,  8.30075741e-02,\n",
              "               2.30854049e-01, -2.31169909e-02, -5.04070744e-02,  1.34752661e-01,\n",
              "               7.66852424e-02,  2.15073675e-02, -1.27813742e-01, -7.30242673e-03,\n",
              "               3.78958322e-02,  8.58590677e-02,  1.36423901e-01, -9.72727090e-02,\n",
              "              -9.41579789e-02,  1.52386472e-01,  5.20205796e-02, -1.03685223e-01,\n",
              "               2.72033699e-02,  1.48735240e-01,  1.88178703e-01, -1.32872626e-01,\n",
              "               1.77185655e-01, -6.41603395e-02,  8.68343562e-02, -1.48525640e-01,\n",
              "               6.93137944e-02, -1.19476439e-02, -5.44782318e-02, -1.56419516e-01,\n",
              "              -1.36232197e-01,  1.06464416e-01, -4.80193645e-02, -6.40646592e-02,\n",
              "               5.20051420e-02,  3.57226329e-03,  1.72116309e-01, -3.96878198e-02,\n",
              "               1.28150016e-01,  7.54405931e-02, -1.89004421e-01,  3.09149362e-02,\n",
              "              -4.19737659e-02, -9.85616967e-02,  4.59401086e-02,  1.50685251e-01,\n",
              "               1.05428383e-01, -5.72489686e-02, -1.53436896e-03,  1.72889873e-01,\n",
              "              -3.19407731e-02,  2.28926942e-01, -8.69583562e-02, -1.16806485e-01,\n",
              "              -5.23191988e-02, -2.00418591e-01, -1.22164123e-01, -5.91899790e-02,\n",
              "              -6.68511614e-02, -2.48277187e-02,  1.56338498e-01,  7.45216310e-02,\n",
              "               2.62560267e-02, -1.41145989e-01,  1.06666438e-01,  8.56582820e-02,\n",
              "              -1.55979991e-02,  9.72828269e-02, -1.17967732e-01,  1.01646796e-01],\n",
              "             dtype=float32)                                                       ],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build LSTM models**"
      ],
      "metadata": {
        "id": "PeV255OoYKt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size + 1, 100, input_length=max_sequence_len - 1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(emdedding_size, activation='linear',  # Adjusted the activation here\n",
        "                kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.compile(loss='mse',  # Changed the loss function to mean squared error\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCJwq6sZ8x8Z",
        "outputId": "08afc818-eab0-4926-d07e-85bd72ee1666"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 10, 100)           29315500  \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 10, 300)           301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 200)               20200     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29797300 (113.67 MB)\n",
            "Trainable params: 29797300 (113.67 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(predictors, label, epochs=500, verbose=1, validation_split=0.2, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "v-Ld-SoCcJRW",
        "outputId": "b1939450-c296-44a4-e551-024c9655ad45"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-0c2df4db6ea3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type Tensor)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wd_NOxGruiEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
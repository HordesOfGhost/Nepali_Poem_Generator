{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ZzRKdGhEk9JL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip96H76Jk1D1",
        "outputId": "bb5daf2c-8636-43e1-e413-f8083113eecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "CHv1tjP0k_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Text**"
      ],
      "metadata": {
        "id": "3PN5-SOhAye8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import the large text**"
      ],
      "metadata": {
        "id": "CzUyWx8M5uEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "print(\"Reading the file .......\")\n",
        "large_text = open(\"/content/drive/MyDrive/nepdata/clean.txt\" , encoding= 'utf-8' , buffering= 10000)\n",
        "lines = large_text.read().strip().split(u\"।\")\n",
        "sentences = [sentence.translate(str.maketrans('', '', string.punctuation)) for sentence in lines]\n",
        "large_text.close()\n",
        "print(f\"Total number of lines in text file {len(sentences)}\")\n",
        "print(f\"Time required to read the file {time.process_time() - start}\")"
      ],
      "metadata": {
        "id": "TEPgyfAclHj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4110b4d8-effb-4118-86f9-59f1d2926f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the file .......\n",
            "Total number of lines in text file 5891518\n",
            "Time required to read the file 101.798553351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import poem set**"
      ],
      "metadata": {
        "id": "pc-jJt515z59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# poem_file = open('/content/drive/MyDrive/poem/datasets/poem.txt','r')\n",
        "# poem = poem_file.read()\n",
        "# poem_corpus = poem.split(\"\\n\")"
      ],
      "metadata": {
        "id": "CrarcgJM56m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Combine**"
      ],
      "metadata": {
        "id": "UCojbYTk6D36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_sentences = sentences + poem_corpus\n",
        "# filtered_combined_sentences = list(filter(lambda x: x != \"\", combined_sentences))"
      ],
      "metadata": {
        "id": "DdXU6zAC6Hx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtered_combined_sentences[-1]"
      ],
      "metadata": {
        "id": "nlfc94ne9qem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess and Tokenize**"
      ],
      "metadata": {
        "id": "W4tI54K-CUb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snowballstemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALxM_niCCTri",
        "outputId": "e38a22b3-1789-4172-8088-2cdc715c01fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snowballstemmer in /usr/local/lib/python3.10/dist-packages (2.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# import snowballstemmer\n",
        "# mainlist = list()\n",
        "# class Main_Data_list:\n",
        "#     def __init__(self, dataset):\n",
        "#         self.dataset = dataset\n",
        "#         self.noise_list = ['\\n','\\ufeff','0','1','2','3','4','5','6','7','8','9','०','१','२','३','४','५','६','७','८','९','१०','।', ',', ';', '?', '!', '—', '-', '.', '\\u200d']\n",
        "#         self.mainlist = []\n",
        "\n",
        "#         self.stemmer = snowballstemmer.NepaliStemmer()\n",
        "\n",
        "#     def simple_tokenizer(self,text) -> list:\n",
        "\n",
        "#         line = re.sub('[।]',\"\", text)\n",
        "\n",
        "#         devanagari_range = r'[\\u0900-\\u097F\\\\]'\n",
        "#         def getDevanagariCharCount(token):\n",
        "#             return len(list(filter(lambda char: re.match(devanagari_range, char), (char for char in token))))\n",
        "#         def isDevanagari(token):\n",
        "#             return True if getDevanagariCharCount(token) >= len(token)/2 else False\n",
        "\n",
        "#         tokens = list(filter(lambda t: isDevanagari(t), line.split(\" \")))\n",
        "#         return tokens\n",
        "\n",
        "#     def get(self):\n",
        "#         for i,line in enumerate(self.dataset[0:2000000]):\n",
        "\n",
        "#             wordsList = self.simple_tokenizer(line)\n",
        "#             words1 = [w for w in wordsList if not w in self.noise_list]\n",
        "#             words = []\n",
        "#             for word in words1:\n",
        "#               words.append([word.replace(noise,'') for noise in self.noise_list][0])\n",
        "#             words  = self.stemmer.stemWords(words)\n",
        "#             if len(words) > 3:\n",
        "#                 self.mainlist.append(words)\n",
        "#             if i % 100000 == 0:\n",
        "#                 print(f\"DONE FOR {i/100000} LAKHS LINES\")\n",
        "#         return self.mainlist\n",
        "\n",
        "# final = Main_Data_list(sentences)\n",
        "# mainlist = final.get()"
      ],
      "metadata": {
        "id": "pm1upcwtNo6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import snowballstemmer\n",
        "mainlist = list()\n",
        "class Main_Data_list:\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.noise_list = ['\\n','\\ufeff','0','1','2','3','4','5','6','7','8','9','०','१','२','३','४','५','६','७','८','९','१०','।', ',', ';', '?', '!', '—', '-', '.', '\\u200d']\n",
        "        self.mainlist = []\n",
        "\n",
        "        self.stemmer = snowballstemmer.NepaliStemmer()\n",
        "\n",
        "    def simple_tokenizer(self,text) -> list:\n",
        "\n",
        "        line = re.sub('[।]',\"\", text)\n",
        "\n",
        "        devanagari_range = r'[\\u0900-\\u097F\\\\]'\n",
        "        def getDevanagariCharCount(token):\n",
        "            return len(list(filter(lambda char: re.match(devanagari_range, char), (char for char in token))))\n",
        "        def isDevanagari(token):\n",
        "            return True if getDevanagariCharCount(token) >= len(token)/2 else False\n",
        "\n",
        "        tokens = list(filter(lambda t: isDevanagari(t), line.split(\" \")))\n",
        "        return tokens\n",
        "\n",
        "    def get(self):\n",
        "        for i,line in enumerate(self.dataset[0:2000000]):\n",
        "\n",
        "            wordsList = self.simple_tokenizer(line)\n",
        "            words1 = [w for w in wordsList if not w in self.noise_list]\n",
        "            words = []\n",
        "            for word in words1:\n",
        "              words.append([word.replace(noise,'') for noise in self.noise_list][0])\n",
        "              self.mainlist.append(words)\n",
        "            if i % 100000 == 0:\n",
        "                print(f\"DONE FOR {i/100000} LAKHS LINES\")\n",
        "        return self.mainlist\n",
        "\n",
        "final = Main_Data_list(sentences)\n",
        "mainlist = final.get()"
      ],
      "metadata": {
        "id": "osKgXSAplNO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24183f91-7618-4dc4-c5c3-686f1cf2b490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE FOR 0.0 LAKHS LINES\n",
            "DONE FOR 1.0 LAKHS LINES\n",
            "DONE FOR 2.0 LAKHS LINES\n",
            "DONE FOR 3.0 LAKHS LINES\n",
            "DONE FOR 4.0 LAKHS LINES\n",
            "DONE FOR 5.0 LAKHS LINES\n",
            "DONE FOR 6.0 LAKHS LINES\n",
            "DONE FOR 7.0 LAKHS LINES\n",
            "DONE FOR 8.0 LAKHS LINES\n",
            "DONE FOR 9.0 LAKHS LINES\n",
            "DONE FOR 10.0 LAKHS LINES\n",
            "DONE FOR 11.0 LAKHS LINES\n",
            "DONE FOR 12.0 LAKHS LINES\n",
            "DONE FOR 13.0 LAKHS LINES\n",
            "DONE FOR 14.0 LAKHS LINES\n",
            "DONE FOR 15.0 LAKHS LINES\n",
            "DONE FOR 16.0 LAKHS LINES\n",
            "DONE FOR 17.0 LAKHS LINES\n",
            "DONE FOR 18.0 LAKHS LINES\n",
            "DONE FOR 19.0 LAKHS LINES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mainlist[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_wi9nuwVzTv",
        "outputId": "885ffb0f-bcb4-4e78-a685-00c64bc21779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['साँवा',\n",
              " 'अक्षर',\n",
              " 'मा',\n",
              " 'बाह्रखरीमिल्नाले',\n",
              " 'भाषा',\n",
              " 'समृद्ध',\n",
              " 'र',\n",
              " 'अर्थपूर्ण',\n",
              " 'बनेजस्तै',\n",
              " 'देशको',\n",
              " 'समाजिक',\n",
              " 'राजनीतिक',\n",
              " 'आर्थिक',\n",
              " 'र',\n",
              " 'अन्य',\n",
              " 'क्षेत्रको',\n",
              " 'स्थूल',\n",
              " 'स्वरूपलाई',\n",
              " 'विवेचनात्मक',\n",
              " 'दृष्टिबाट',\n",
              " 'अथ्र्याइएमा',\n",
              " 'मात्र',\n",
              " 'जनताले',\n",
              " 'देशको',\n",
              " 'गति',\n",
              " 'र',\n",
              " 'शासनको',\n",
              " 'नियत',\n",
              " 'बुझ्ने',\n",
              " 'विश्वास',\n",
              " 'लिएर',\n",
              " 'त्यसको',\n",
              " 'भित्री',\n",
              " 'एवं',\n",
              " 'समग्र',\n",
              " 'स्वरूप',\n",
              " 'जनता',\n",
              " 'समक्ष',\n",
              " 'प्रस्तुत',\n",
              " 'गर्ने',\n",
              " 'जमर्कोमा',\n",
              " 'यो',\n",
              " 'अनलाइन',\n",
              " 'खबर',\n",
              " 'पत्रिका',\n",
              " 'बाह्रखरी',\n",
              " 'उपस्थित',\n",
              " 'भएको',\n",
              " 'छ',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model**"
      ],
      "metadata": {
        "id": "HxLikQOHCkpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "model = gensim.models.Word2Vec(\n",
        "    vector_size = 200 ,\n",
        "    window=  5,\n",
        "    min_count=2,\n",
        "    workers= 4\n",
        ")\n",
        "\n",
        "model.build_vocab(mainlist, progress_per=1000 )\n",
        "\n",
        "model.train(mainlist, total_examples= model.corpus_count, epochs= model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucxE8MjcCkHC",
        "outputId": "41020fca-2fa3-48c1-e87c-68c15ea3fa23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128517190, 161130595)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('इच्छा')"
      ],
      "metadata": {
        "id": "V_tG5xijleTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21c96d6-f7ad-4511-fd38-3d4a50cf94d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('चाहना', 0.7057510018348694),\n",
              " ('रहर', 0.5624958872795105),\n",
              " ('आकांक्षा', 0.5372521877288818),\n",
              " ('अठोट', 0.5326061248779297),\n",
              " ('सपना', 0.5265827178955078),\n",
              " ('आँट', 0.5181187391281128),\n",
              " ('हिम्मत', 0.5171414613723755),\n",
              " ('ईच्छा', 0.5130905508995056),\n",
              " ('चाहाना', 0.5074719786643982),\n",
              " ('रुचि', 0.5058373212814331)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('रानी')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrDGeWWzf1Xr",
        "outputId": "46f5e3a3-d046-48bb-c3a8-74857c7308e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('यम्पावती', 0.631863534450531),\n",
              " ('खड्गशमशेर', 0.6235966086387634),\n",
              " ('काली', 0.6084131002426147),\n",
              " ('तेजकुमारी', 0.6041477918624878),\n",
              " ('हनुमान', 0.5939910411834717),\n",
              " ('शुद्धोधन', 0.5907953977584839),\n",
              " ('चाँदनी', 0.5906463265419006),\n",
              " ('माता', 0.5868694186210632),\n",
              " ('शंख', 0.5859056711196899),\n",
              " ('भैरव', 0.5850088000297546)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('आँसुमा')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "dOvFgtAFstER",
        "outputId": "c1db051c-9afd-4878-eb71-0b729d119be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0d770abfc17a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'आँसुमा'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'आँसुमा' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"nepaliW2V_5Million.model\")"
      ],
      "metadata": {
        "id": "5mwJIHhoGZoP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u71mzdEVpoJM"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYXlXNlsplb9",
        "outputId": "7f422036-d1b1-474b-c311-7f52a29e9a41"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "je-g-H8UwXJq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102qIx0ns3Dh"
      },
      "source": [
        "# Creating Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDB4s_fcptTz",
        "outputId": "c4b7afef-6e5b-400f-8c49-6064246a3f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: [36, 51, 38, 53, 59, 31, 45, 36, 59, 40, 46, 40, 51, 12, 59, 28, 45, 32, 45, 59, 44, 53, 59, 60]\n",
            "Converted back to characters: मेरो नाम विवेक थापा हो ।\n"
          ]
        }
      ],
      "source": [
        "class NepaliTokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dictionary = {}\n",
        "        self.reverse_dictionary = {}\n",
        "\n",
        "        # Add the padding token\n",
        "        self.__add_to_dict('<pad>')\n",
        "\n",
        "        # Add Nepali characters to the dictionary\n",
        "        nepali_characters = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ',\n",
        "                             'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n",
        "                             'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', 'ा', 'ि',\n",
        "                             'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ो', 'ौ', '्' , '!', '.', ',', ' ', '।' ]\n",
        "\n",
        "        for char in nepali_characters:\n",
        "            self.__add_to_dict(char)\n",
        "\n",
        "    def __add_to_dict(self, character):\n",
        "        if character not in self.dictionary:\n",
        "            self.dictionary[character] = len(self.dictionary)\n",
        "            self.reverse_dictionary[self.dictionary[character]] = character\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return [self.dictionary.get(c, self.dictionary['<pad>']) for c in text]  # Use '<pad>' token for unknown characters\n",
        "\n",
        "    def character_to_token(self, character):\n",
        "        return self.dictionary.get(character, self.dictionary['<pad>'])\n",
        "\n",
        "    def token_to_character(self, token):\n",
        "        return self.reverse_dictionary.get(token, '<pad>')\n",
        "\n",
        "    def left_pad_tokens(self, tokens, max_length):\n",
        "        padded_tokens = np.pad(tokens, (max_length - len(tokens), 0), 'constant', constant_values=0)\n",
        "        return padded_tokens.tolist()\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.dictionary)\n",
        "\n",
        "# Example usage\n",
        "nepali_tokenizer = NepaliTokenizer()\n",
        "text = \"मेरो नाम विवेक थापा हो ।\"\n",
        "tokens = nepali_tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens back to characters\n",
        "characters = [nepali_tokenizer.token_to_character(token) for token in tokens]\n",
        "print(\"Converted back to characters:\", ''.join(characters))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6exEArkrtmyx"
      },
      "source": [
        "# Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8YdfHFb-q7Wm"
      },
      "outputs": [],
      "source": [
        "poem_file = open('C:/Users/Ghost/Desktop/gits/Nepali_Poem_Generator/datasets/poem.txt','r')\n",
        "poem = poem_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2qRP_Q1Qt9hT"
      },
      "outputs": [],
      "source": [
        "def remove_noise(sentences):\n",
        "    punctuations = ['\\n','\\ufeff','0','1','2','3','4','5','6','7','8','9','०','१','२','३','४','५','६','७','८','९','१०','\\u200d']\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        for punct in punctuations:\n",
        "            sentence = sentence.replace(punct,'')\n",
        "        processed_sentences.append(sentence)\n",
        "\n",
        "    return processed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3WXAgCEuAnV",
        "outputId": "3175412b-9587-434e-ad68-e8f17d5301c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,', 'मनको वनमा ननिभ्ने गरी विरह जलाई !', 'ननिभ्ने गरी विरह जलाई,', 'लोचनका तारा ! हे मेर प्यारा ! यो जोति  बिलाए !', 'के भनूँ? भन्ने म केही थिइन  विष नै पिलाए !']\n",
            "['नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,', 'मनको वनमा ननिभ्ने गरी विरह जलाई !', 'ननिभ्ने गरी विरह जलाई,', 'लोचनका तारा ! हे मेर प्यारा ! यो जोति  बिलाए !', 'के भनूँ? भन्ने म केही थिइन  विष नै पिलाए !']\n"
          ]
        }
      ],
      "source": [
        "poem_corpus = poem.split(\"\\n\")\n",
        "print(poem_corpus[:5])\n",
        "\n",
        "processed_poem_corpus = remove_noise(poem_corpus)\n",
        "print(processed_poem_corpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKupEVCBuFtP"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nP3AHVfvuDoB"
      },
      "outputs": [],
      "source": [
        "tokenized_poem_data = []\n",
        "for poem in processed_poem_corpus:\n",
        "  tokenized_poem_data.append(nepali_tokenizer.tokenize(poem))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI-MOP7juO_q",
        "outputId": "73efad16-27b3-42e4-c425-1af01f6c182e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,\n",
            "[31, 18, 45, 24, 47, 59, 19, 45, 31, 53, 43, 55, 59, 44, 51, 59, 36, 51, 38, 45, 59, 32, 55, 38, 45, 26, 59, 56, 59, 1, 12, 51, 39, 47, 59, 36, 39, 45, 4, 58]\n",
            "Converted back to characters: नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,\n"
          ]
        }
      ],
      "source": [
        "print(processed_poem_corpus[0])\n",
        "print(tokenized_poem_data[0])\n",
        "\n",
        "# Convert tokens back to characters\n",
        "characters = [nepali_tokenizer.token_to_character(token) for token in tokenized_poem_data[0]]\n",
        "print(\"Converted back to characters:\", ''.join(characters))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGXfCCwuvRIQ"
      },
      "source": [
        "# Padding the Tokenized Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OXW21Md2ukvy"
      },
      "outputs": [],
      "source": [
        "# Example of left-padding the tokens\n",
        "max_length = max([len(x) for x in tokenized_poem_data])\n",
        "\n",
        "for index, tokens in enumerate(tokenized_poem_data):\n",
        "    tokenized_poem_data[index] = nepali_tokenizer.left_pad_tokens(tokens, max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ZwpDs0xJ9R",
        "outputId": "101c5357-828b-447f-dba7-e5e980f3fa51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 18, 45, 24, 47, 59, 19, 45, 31, 53, 43, 55, 59, 44, 51, 59, 36, 51, 38, 45, 59, 32, 55, 38, 45, 26, 59, 56, 59, 1, 12, 51, 39, 47, 59, 36, 39, 45, 4, 58]\n",
            "Converted back to characters: <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>नछाडी जानोस् हे मेरा प्राण ! अकेली मलाई,\n"
          ]
        }
      ],
      "source": [
        "print(processed_poem_corpus[0])\n",
        "print(tokenized_poem_data[0])\n",
        "\n",
        "# Convert tokens back to characters\n",
        "characters = [nepali_tokenizer.token_to_character(token) for token in tokenized_poem_data[0]]\n",
        "print(\"Converted back to characters:\", ''.join(characters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBSb1vNxypJ2"
      },
      "source": [
        "# Input Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SbBEyu_qxSZe"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch module that converts tokens into embeddings.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length)\n",
        "    Output dimension is: (batch_size, sequence_length, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, number_of_tokens):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = torch.nn.Embedding(\n",
        "            num_embeddings=number_of_tokens,\n",
        "            embedding_dim=d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGmvcnGF2udw"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PUS4VQNH1TpG"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the\n",
        "    transformer's input embeddings to provide a sense of position of the sequence elements.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.positional_encoding = self.create_positional_encoding()\n",
        "\n",
        "    def create_positional_encoding(self):\n",
        "        \"\"\"\n",
        "        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize positional encoding matrix\n",
        "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
        "\n",
        "        # Calculate positional encoding for each position and each dimension\n",
        "        for pos in range(self.max_sequence_length):\n",
        "            for i in range(0, self.d_model, 2):\n",
        "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
        "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
        "\n",
        "                if i + 1 < self.d_model:\n",
        "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
        "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
        "\n",
        "        # Convert numpy array to PyTorch tensor and return it\n",
        "        return torch.from_numpy(positional_encoding).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds the positional encoding to the input embeddings at the corresponding positions.\n",
        "        \"\"\"\n",
        "        # Add positional encodings to input embeddings. The \":\" indexing ensures we only add positional encodings up\n",
        "        # to the length of the sequence in the batch. x.size(0) is the batch size, so this is a way to make sure\n",
        "        # we're not adding extra positional encodings.\n",
        "        return x + self.positional_encoding[:x.size(1), :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yJfP2l3i7s"
      },
      "source": [
        "# Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1VfHa2Ju2sXK"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a self attention layer.\n",
        "    This layer is used in the MultiHeadedSelfAttention module.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, head_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, head_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = head_dimension\n",
        "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the self attention.\n",
        "\n",
        "        x dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "        output dimension is: (batch_size, sequence_length, head_dimension)\n",
        "        mask dimension is: (batch_size, sequence_length)\n",
        "\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n",
        "        query = self.query_layer(x)\n",
        "        key = self.key_layer(x)\n",
        "        value = self.value_layer(x)\n",
        "\n",
        "        # Calculate the attention weights.\n",
        "        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n",
        "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "        # Scale the attention weights.\n",
        "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
        "\n",
        "        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n",
        "        # This will make the softmax output 0 for these values.\n",
        "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
        "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n",
        "        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n",
        "        attention_scores = self.softmax(attention_weights)\n",
        "\n",
        "        # The attention scores are multiplied by the value\n",
        "        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n",
        "        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n",
        "        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n",
        "        return torch.bmm(attention_scores, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Za8c1V6cj9"
      },
      "source": [
        "# MultiHead Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pTMtX4LV6af2"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a multi head attention layer.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, number_of_heads):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = embedding_dimension // number_of_heads\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        # Create the self attention modules\n",
        "        self.self_attentions = torch.nn.ModuleList(\n",
        "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
        "\n",
        "        # Create a linear layer to combine the outputs of the self attention modules\n",
        "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the multi head attention.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        mask dimensions are: (batch_size, sequence_length)\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "        # Compute the self attention for each head\n",
        "        # self_attention_outputs dimensions are:\n",
        "        # (number_of_heads, batch_size, sequence_length, head_dimension)\n",
        "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
        "\n",
        "        # Concatenate the self attention outputs\n",
        "        # self_attention_outputs_concatenated dimensions are:\n",
        "        # (batch_size, sequence_length, number_of_heads * head_dimension)\n",
        "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
        "\n",
        "        # Apply the output layer to the concatenated self attention outputs\n",
        "        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        return self.output_layer(concatenated_self_attention_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0AIWz7n-fa2"
      },
      "source": [
        "# Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "292Ysesq7RV9"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for an encoder layer.\n",
        "\n",
        "    An encoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
        "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n",
        "        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the encoder layer.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        mask dimensions are: (batch_size, sequence_length)\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # Layer normalization 1\n",
        "        normalized_x = self.layer_normalization_1(x)\n",
        "\n",
        "        # Multi headed self attention\n",
        "        attention_output = self.multi_headed_self_attention(normalized_x, mask)\n",
        "\n",
        "        # Residual output\n",
        "        residual_output = x + attention_output\n",
        "\n",
        "        # Layer normalization 2\n",
        "        normalized_residual_output = self.layer_normalization_2(residual_output)\n",
        "\n",
        "        # Feed forward\n",
        "        feed_forward_output = self.feed_forward(normalized_residual_output)\n",
        "\n",
        "        # Dropout, only when training.\n",
        "        if self.training:\n",
        "            feed_forward_output = self.dropout(feed_forward_output)\n",
        "\n",
        "        # Residual output\n",
        "        return residual_output + feed_forward_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7uuPRqS0-hEi"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a stack of decoders.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_layers,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate,\n",
        "            max_sequence_length\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "        # Create the encoder layers\n",
        "        self.encoder_layers = torch.nn.ModuleList(\n",
        "            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
        "             range(number_of_layers)])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        decoder_outputs = x\n",
        "        for decoder_layer in self.encoder_layers:\n",
        "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
        "\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NvmK93zH-oCZ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a feed forward layer.\n",
        "\n",
        "    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
        "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the feed forward layer.\n",
        "        \"\"\"\n",
        "        return self.linear_2(torch.relu(self.linear_1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yacQ2qY1-v2p"
      },
      "source": [
        "# Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KgSdSIDJ-qco"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a language model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            number_of_tokens,  # The number of tokens in the vocabulary\n",
        "            max_sequence_length=512,  # The maximum sequence length to use for attention\n",
        "            embedding_dimension=512,  # The dimension of the token embeddings\n",
        "            number_of_layers=6,  # The number of decoder layers to use\n",
        "            number_of_heads=4,  # The number of attention heads to use\n",
        "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
        "            dropout_rate=0.1  # The dropout rate to use\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.number_of_tokens = number_of_tokens\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        if feed_forward_dimension is None:\n",
        "            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n",
        "            self.feed_forward_dimension = embedding_dimension * 4\n",
        "        else:\n",
        "            self.feed_forward_dimension = feed_forward_dimension\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Create the token embedding layer\n",
        "        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n",
        "\n",
        "        # Create the positional encoding layer\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
        "\n",
        "        # Create the normalization layer\n",
        "        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "        # Create the decoder stack\n",
        "        self.decoder = DecoderStack(\n",
        "            embedding_dimension=embedding_dimension,\n",
        "            number_of_layers=number_of_layers,\n",
        "            number_of_heads=number_of_heads,\n",
        "            feed_forward_dimension=self.feed_forward_dimension,\n",
        "            dropout_rate=dropout_rate,\n",
        "            max_sequence_length=max_sequence_length\n",
        "        )\n",
        "\n",
        "        # Create the language model head\n",
        "        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Compute the token embeddings\n",
        "        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "\n",
        "        # Compute the positional encoding\n",
        "        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        positional_encoding = self.positional_encoding(token_embeddings)\n",
        "\n",
        "        # Post embedding layer normalization\n",
        "        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n",
        "\n",
        "        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n",
        "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
        "\n",
        "        return lm_head_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d5yKOmb3-xsZ"
      },
      "outputs": [],
      "source": [
        "class LMHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for the language model head.\n",
        "    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, number_of_tokens):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_tokens = number_of_tokens\n",
        "        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the language model head.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
        "        \"\"\"\n",
        "        # Compute the linear layer\n",
        "        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
        "        linear_output = self.linear(x)\n",
        "\n",
        "        return linear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bFSsb2UB8xh"
      },
      "source": [
        "# Autoregressive Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LiL289bRBHk6"
      },
      "outputs": [],
      "source": [
        "class AutoregressiveWrapper(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module that wraps a GPT model and makes it autoregressive.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gpt_model):\n",
        "        super().__init__()\n",
        "        self.model = gpt_model\n",
        "        self.max_sequence_length = self.model.max_sequence_length\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Autoregressive forward pass\n",
        "        \"\"\"\n",
        "        inp, target = x[:, :-1], x[:, 1:]\n",
        "        mask = mask[:, :-1]\n",
        "\n",
        "        output = self.model(inp, mask)\n",
        "        return output, target\n",
        "\n",
        "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Calculate the token probabilities for the next token in the sequence.\n",
        "        \"\"\"\n",
        "        logits = self.model(x, mask)[:, -1]\n",
        "\n",
        "        # Apply the temperature\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "        # Apply the softmax\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        return probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv5AOb75CFSi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZVH5dKVaCofZ"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, tokenizer: NepaliTokenizer, optimizer=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        if optimizer is None:\n",
        "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        else:\n",
        "            self.optimizer = optimizer\n",
        "        self.tokenizer = tokenizer\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, data: List[str], epochs, batch_size):\n",
        "        loss_per_epoch = []\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "\n",
        "            # Shuffle the sequences\n",
        "            random.shuffle(data)\n",
        "\n",
        "            # Create batches of sequences and their respective mask.\n",
        "            batches = []\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
        "\n",
        "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
        "                mask_tensor = torch.ones_like(sequence_tensor)\n",
        "                mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<pad>')] = 0\n",
        "\n",
        "                batches.append((sequence_tensor, mask_tensor))\n",
        "\n",
        "            # Train the model on each batch\n",
        "            for batch in batches:\n",
        "                self.model.train()\n",
        "\n",
        "                # Create the input and mask tensors\n",
        "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "\n",
        "                for i, input_entry in enumerate(batch[0]):\n",
        "                    input_tensor[i] = input_entry\n",
        "\n",
        "                for i, mask_entry in enumerate(batch[1]):\n",
        "                    mask_tensor[i] = mask_entry\n",
        "\n",
        "                # Compute the model output\n",
        "                model_output, target = self.model.forward(x=input_tensor, mask=mask_tensor)\n",
        "\n",
        "                # Compute the losses\n",
        "                # The loss is computed on the model output and the target\n",
        "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
        "\n",
        "                # Backpropagate the loss.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the gradients. This is used to prevent exploding gradients.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "\n",
        "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
        "                # are not used in the next step.\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            # Print the loss\n",
        "            epoch_loss = np.average(losses)\n",
        "            loss_per_epoch.append(epoch_loss)\n",
        "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
        "\n",
        "        return loss_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Juqsj8dsB73p",
        "outputId": "e493c2d4-a20a-4633-8c9b-a34dcfa5fab8"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m     43\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, tokenizer, optimizer)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[22], line 60\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, data, epochs, batch_size)\u001b[0m\n\u001b[0;32m     57\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Update the model parameters. This is done by taking a step in the direction of the gradient.\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Reset the gradients. This is done so that the gradients from the previous batch\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# are not used in the next step.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
        "    # Create sequences of length max_sequence_length + 1\n",
        "    # The last token of each sequence is the target token\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
        "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
        "    # Tokenize the training data\n",
        "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
        "    for _ in range(max_sequence_length):\n",
        "        # Prepend padding tokens\n",
        "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
        "    return tokenized_training_data\n",
        "\n",
        "\n",
        "tokenizer = NepaliTokenizer()\n",
        "\n",
        "embedding_dimension = 256\n",
        "max_sequence_length = 20\n",
        "number_of_tokens = tokenizer.size()\n",
        "\n",
        "# Create the model\n",
        "model = AutoregressiveWrapper(LanguageModel(\n",
        "    embedding_dimension=embedding_dimension,\n",
        "    number_of_tokens=number_of_tokens,\n",
        "    number_of_heads=4,\n",
        "    number_of_layers=3,\n",
        "    dropout_rate=0.1,\n",
        "    max_sequence_length=max_sequence_length\n",
        "))\n",
        "\n",
        "# Create the training data\n",
        "training_data = '. '.join(processed_poem_corpus)\n",
        "\n",
        "tokenized_and_padded_training_data = tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\n",
        "sequences = create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n",
        "\n",
        "# Train the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "trainer = Trainer(model, tokenizer, optimizer)\n",
        "trainer.train(sequences, epochs=100, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpnxRQ0oC-Iv"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(self, path):\n",
        "    print(f'Saving checkpoint {path}')\n",
        "    torch.save({\n",
        "        'number_of_tokens': self.number_of_tokens,\n",
        "        'max_sequence_length': self.max_sequence_length,\n",
        "        'embedding_dimension': self.embedding_dimension,\n",
        "        'number_of_layers': self.number_of_layers,\n",
        "        'number_of_heads': self.number_of_heads,\n",
        "        'feed_forward_dimension': self.feed_forward_dimension,\n",
        "        'dropout_rate': self.dropout_rate,\n",
        "        'model_state_dict': self.state_dict()\n",
        "    }, path)\n",
        "\n",
        "@staticmethod\n",
        "def load_checkpoint(path) -> 'LanguageModel':\n",
        "    checkpoint = torch.load(path)\n",
        "    model = LanguageModel(\n",
        "        number_of_tokens=checkpoint['number_of_tokens'],\n",
        "        max_sequence_length=checkpoint['max_sequence_length'],\n",
        "        embedding_dimension=checkpoint['embedding_dimension'],\n",
        "        number_of_layers=checkpoint['number_of_layers'],\n",
        "        number_of_heads=checkpoint['number_of_heads'],\n",
        "        feed_forward_dimension=checkpoint['feed_forward_dimension'],\n",
        "        dropout_rate=checkpoint['dropout_rate']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n",
        "\n",
        "def save_checkpoint(self, path):\n",
        "    self.model.save_checkpoint(path)\n",
        "\n",
        "@staticmethod\n",
        "def load_checkpoint(path) -> 'AutoregressiveWrapper':\n",
        "    model = LanguageModel.load_checkpoint(path)\n",
        "    return AutoregressiveWrapper(model)\n",
        "\n",
        "model.save_checkpoint('./trained_model')\n",
        "model = model.load_checkpoint('./trained_model')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
